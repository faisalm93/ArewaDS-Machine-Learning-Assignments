{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this Assignment, I decided to follow the adequate rubric criteria where i adopted an existing algorith from the internet and create a simple grid environment represented by a 5x5 numpy array.\n",
        "\n",
        "The agent's goal is to navigate from the top-left corner to the bottom-right corner while avoiding obstacles represented by -1 and reaching the goal represented by +1.\n",
        "\n",
        "The Q-table is initialized with zeros, and the Q-Learning algorithm is performed over a specified number of episodes.\n",
        "\n",
        "In each episode, the agent chooses an action based on the epsilon-greedy policy (either exploration or exploitation) and after taking an action, the agent updates the Q-table using the Q-Learning update rule.\n",
        "\n",
        "The learning rate (alpha), discount factor (gamma), and exploration rate (epsilon) are hyperparameters that can be adjusted to control the learning behavior. At the end of the learning process, the learned Q-table is printed.\n"
      ],
      "metadata": {
        "id": "9V680xwQEHX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment\n",
        "environment = np.array([[0, 0, 0, 0, 0],\n",
        "                       [0, -1, 0, -1, 0],\n",
        "                       [0, 0, 0, -1, 0],\n",
        "                       [0, -1, -1, 0, 0],\n",
        "                       [0, 0, 0, 0, 0]])\n",
        "\n",
        "# Define the Q-table\n",
        "q_table = np.zeros((5, 5))\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration rate\n",
        "\n",
        "# Perform Q-Learning\n",
        "episodes = 1000\n",
        "for episode in range(episodes):\n",
        "    state = (0, 0)  # Starting state\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Choose an action (exploitation or exploration)\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = np.random.randint(0, 4)  # Exploration\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Exploitation\n",
        "\n",
        "        # Perform the chosen action and observe the next state and reward\n",
        "        if action == 0:  # Up\n",
        "            next_state = (state[0] - 1, state[1])\n",
        "        elif action == 1:  # Down\n",
        "            next_state = (state[0] + 1, state[1])\n",
        "        elif action == 2:  # Left\n",
        "            next_state = (state[0], state[1] - 1)\n",
        "        elif action == 3:  # Right\n",
        "            next_state = (state[0], state[1] + 1)\n",
        "\n",
        "        reward = environment[next_state]\n",
        "\n",
        "        # Update the Q-table\n",
        "        q_table[state][action] = (1 - alpha) * q_table[state][action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # Check if the episode is finished\n",
        "        if reward == -1 or reward == 1:\n",
        "            done = True\n",
        "\n",
        "# Print the learned Q-table\n",
        "print(\"Learned Q-table:\")\n",
        "print(q_table)\n"
      ],
      "metadata": {
        "id": "uVuHrGDMGICA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}